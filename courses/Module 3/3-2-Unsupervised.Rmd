---
title: "Unsupervised Learning  <br> <br> <br>"
author: "Omni Analytics Group"
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    css: [default, default-fonts, classroom.css]
    lib_dir: libs
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.height = 6, fig.align = "center")

```

## Outline

- Group the ethereum holders dataset using k means clustering
- Evaluation metrics and analysis of number of clusters
- Visualization of clusters
- Other Methods: k means, pam, hclust - dendograms?

---

## Glossary

- Observation
- Cluster

---

## Objective

The marketing team would like to understand more about the kinds of Ethereum holders among the customers. They are wondering if there are specific profiles within the customer base that distinguish the ethereum holders from each other. 

Our objective:
- One way of understanding the kinds of Ethereum holders is to categorize them into groups, based on the similarity of characteristics between customers.  

Since the marketing team do not already have defined profiles that they are looking to identify, this is an unlabelled dataset. When given unlabelled data, we use unsupervised methods to cluster or find groups within the dataset.

We will go through 3 different clustering techniques to help the marketing team.

---

## Dataset Preparation

Let us read in the dataset, filter it to take only the Ethereum owners and prepare it by doing the following:

- create a numeric donated column that has a value of 1 if donated == TRUE and 0 if donated == FALSE
- select only the numeric columns
- omit all rows that have NA values


```{r, message = FALSE, warning = FALSE}
library(tidyverse)

load("data_new.RDS")

data_clus <- data_new %>%
  filter(owns_ethereum == "TRUE") %>%
  mutate(donated = as.numeric(as.logical(donated))) %>% # change to 0 and 1 
  select(where(is.numeric)) %>% # select only the numeric values
  na.omit()

```

---

## K means clustering

K means is a technique that groups data points or observations into k groups in the data. The data is grouped based on feature similarity.

The algorithm allocates k centroids to the data, and tries to minimize the distance from each data point to the centroid. In other words it tries to find the most similar data points around a representative centroid.

The formal algorithm is as follows:

- Choose the number of clusters k
- Select k random points from the data as cluster centroids
- Assign all the other surrounding data points to the closest centroid 
- Recompute the centroids of newly formed clusters
- Repeat the 3rd and 4th steps till the data points remain in the same cluster/ till the centroids do not change. 
  
Alternatively we can have a stopping criteria of maximum iterations. For e.g, 100 iterations 

---

## Normalization

The variables are of differing units. For e.g, age has a range of ~17 to 90 years, where as donated is just an indicator variable of 0 and 1. When using analysis techniques like clustering we need to make sure each variable has the same influence.

Hence, we normalize all the numeric values.

```{r}
norm_mat <- as.data.frame(sapply(data_clus, function(x) { (x - min(x)) / (max(x) - min(x)) }))
```

---

## Deciding the number of clusters

'k' is commonly decided by trying different numbers of groups. We decide to decrease k if there are many highly similar groups or increase the k if we think there should be more variety of groups.

Another way to visually decide on the number of clusters is to use the 'elbow method', where we create multiple clusters and extract the total within-cluster sum of squares value from each model. 

We want to know the value of 'k' that decreases the within-cluster sum of squares the most (has the clusters with individuals most like each other), at the same time the clusters are not redundant. This is commonly found at the elbow of the plot, which is the last value of k before the slope levels off.

```{r eval=FALSE}
# run many models with varying value of k 
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = data_clus, centers = k)
  model$tot.withinss
})
# create a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)
# make the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() + geom_point()+
  scale_x_continuous(breaks = 1:10)
```

---

## K means

Now using the optimal k value, we carry out a k means cluster analysis on the customer dataset.

```{r}
set.seed(1234)

myclust <- kmeans(x = norm_mat, centers = 3)

# add the cluster assignment
data_clus$kmeans <- myclust$cluster

```

---

### Tabular Cluster Summary

One way to visualize the clusters is by using tables.

```{r}
## Produce group averages
data_clus %>%
    group_by(kmeans) %>%
    mutate(Size = n()) %>%
    summarise_all(funs(mean)) %>%
    round()
```

---

### Parallel Coordinate Plot

This is another way to create side by side visualizations to compare the average feature values in each cluster
Similar to how we needed to scale the data before clustering, we need to make sure the values are of asimilar scale so that we can compare oroportions


```{r}
norm_mat$kmeans <- myclust$cluster 

scaled_clus_avg <- norm_mat %>%
    mutate(kmeans = myclust$kmeans) %>%
    group_by(kmeans) %>%
    mutate(Size = n()) %>%
    unite(col = kmeans, kmeans, Size, sep = " size:") %>%
    group_by(kmeans) %>%
    summarise_all(funs(mean))

```

---

```{r eval=FALSE}
melted_df <- scaled_clus_avg %>%
    gather(key=variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(kmeans),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    #facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")

```

---
```{r echo=FALSE}
#PCP
melted_df <- scaled_clus_avg %>% 
    gather(key = variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(kmeans),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")

```

---

### Pair plots

In some cases we can use pair plots to visualize groups that are well separated.

```{r}
library(GGally)
data_clus %>%
    ggpairs(columns = 1:6, aes(color = as_factor(kmeans), alpha = 0.5))
```

---

## PAM

The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints instead of l2.

k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.

It is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.

```{r}
library(cluster)
myclust <- pam(norm_mat, k = 3)

scaled_clus_avg <- norm_mat %>%
    mutate(pam = myclust$pam) %>%
    group_by(pam) %>%
    mutate(Size = n()) %>%
    unite(col = pam, pam, Size, sep = " size:") %>%
    group_by(pam) %>%
    summarise_all(funs(mean))

melted_df <- scaled_clus_avg %>%
    gather(key = variable, value = value, 2:(ncol(.) - 1)) %>%
    group_by(pam, variable) %>%
    summarise(value = mean(value))
ggplot(melted_df, aes(variable, value, color = pam, group = pam)) + 
    geom_line(size = 1.3) + 
    theme_bw() +
    ylim(c(0, 1))
```

---

## HCLUST

```{r}
hclust_complete <- norm_mat %>%
  dist() %>%
  hclust(method = "complete")

# hclust_average <- data_clus %>%
#   dist() %>%
#   hclust(method = "average")
# 
# hclust_single <- data_clus %>%
#   dist() %>%
#   hclust(method = "single")


clusters <- cutree(hclust_complete, k = 4)

data_clus$hclust <- factor(clusters)

```

---

```{r}
melted_df <- data_clus %>%
    gather(key = variable, value = value, 1:(ncol(.) - 3)) %>%
    group_by(hclust, variable) %>%
    summarise(value = mean(value))
ggplot(melted_df, aes(variable, value, color = hclust, group = hclust)) + 
    geom_line() + 
    theme_bw() +
    ylim(c(0, 1))
#library(factoextra) #factoextra provides functions (fviz_dend()) to visualize the clustering created using hclust(). We use fviz_dend() to show the dendrogram.

#factoextra::fviz_dend(hclust_complete, main = "complete", k = 2)
```

---

## Your Turn

The marketing team loved the groups of ethereum holders. Now they want to segment the entire customer base to find unique groups. 

Task: Find well separated groups using the entire dataset.

- How many different clusters have you settled at?
- Are you able to see which features distinguish the ethereum holders from the non ethereum holders?

---

## Answer

```{r}
## read in data
load("data_new.RDS")

data_clus <- data_new %>%
  mutate(donated = as.numeric(as.logical(donated))) %>% # change to 0 and 1 
  select(where(is.numeric)) %>% # --TODO save this in a csv so that user doesn't filter
  na.omit()

## Clustering
set.seed(234)

# normalize numeric values
norm_mat <- as.data.frame(sapply(data_clus, function(x) { (x - min(x)) / (max(x) - min(x)) }))

myclust <- kmeans(x = norm_mat, centers = 5)

summary(myclust)

# add the cluster assignment
data_clus$Cluster <- myclust$cluster

## Produce group averages
data_clus %>%
  group_by(Cluster) %>%
    mutate(Size = n()) %>%
    summarise_all(funs(mean)) %>%
    round()
## Viz
#PCP
norm_mat$Cluster <- myclust$cluster 

scaled_clus_avg <- norm_mat %>%
    group_by(Cluster) %>%
    mutate(Size = n()) %>%
    unite(col = Cluster, Cluster, Size, sep = " size:") %>%
    group_by(Cluster) %>%
    summarise_all(funs(mean))

melted_df <- scaled_clus_avg %>% 
    gather(key=variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(Cluster),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

```
---
```{r, echo=FALSE}
ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")
```

