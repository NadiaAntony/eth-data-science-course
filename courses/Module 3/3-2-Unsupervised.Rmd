---
title: "Unsupervised Learning  <br> <br> <br>"
author: ""
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    css: [default, default-fonts, classroom.css]
    lib_dir: libs
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center")

```

# Contents

- Group the ethereum holders dataset using k means clustering
- Evaluation metrics and analysis of number of clusters
- Visualization of clusters
- Other Methods: k means, pam, hclust - dendograms?

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

## Obective

Survey data of adults adapted from the census. We would like to understand characteristics and guess at who might be the ethereum holders in the data!

When given unlabelled data, we use unsupervised methods to try and segment or find groups within the dataset.

---

## Dataset



```{r}
load("data_new.RDS")

data <- data_new

```

## Ethereum holder profile

We can look at online surveys to see what might be an ideal ethereum holder profile. 
If we do not currently have intuition about the data, this is an easy way to build some background or domain knowledge.

Ideal profile: 

- 70% are Male
- 58% are aged under 34 or between 18-44
- 82% have a Bachelorâ€™s degree or higher 
- 36% have an annual income over US$100k

Let us see what are the different groups of adults in this dataset. In order to do this we will need to learn some unsupervised clustering techniques

---

## K means clustering

K means is a technique that groups data points or observations into k groups in the data. The data is grouped based on feature similarity.

The algorithm allocates k centroids to the tries to minimize the distance from each data point to the centroid. In other words it tries to find the most similar data points around a representative centroid.

The formal algorithm is as follows:

- Choose the number of clusters k
- Select k random points from the data as cluster centroids
- Assign all the other surrounding data points to the closest centroid
- Recompute the centroids of newly formed clusters
- Repeat the 3rd and 4th steps till the data points remain in the same cluster/ till the centroids do not change. 
  
Alternatively we can have a stopping criteria of maximum iterations. For e.g, 100 iterations 

---

## Similarity

The concept of similarity is important in understanding how the input data is grouped. Each data point can be visualized as a vector or a collection of numeric values.


---

## K means

Here we carry out a k means analysis using the numeric variables

```{r}
# normalize numeric values
set.seed(1234)
data_clus <- data %>% 
  rowid_to_column() %>%
  #filter(owns_ethereum == "TRUE") %>%
  select(where(is.numeric)) %>%
  na.omit()
mymat <- scale(data_clus[,-1])

myclust <- kmeans(x = mymat, centers = 6)

summary(myclust)

data_clus$Cluster <- myclust$cluster

```

---

## Evaluation

---

## Viz

```{r}
mymat <- as.data.frame(sapply(data_clus[,-1], function(x) { (x - min(x)) / (max(x) - min(x)) }))
mymat$Cluster <- myclust$cluster 

## Produce group averages
data_clus %>%
    group_by(Cluster) %>%
    mutate(Size = n()) %>%
    summarise_all(funs(mean)) %>%
    round()

```

---

### Principal Coordinate Plot

- Way to create side by side visualizations to compare the average feature values in each cluster

Scaling to create comparable plots


```{r}
scaled_clus_avg <- mymat %>%
    group_by(Cluster) %>%
    mutate(Size = n()) %>%
    unite(col = Cluster, Cluster, Size, sep = " size:") %>%
    group_by(Cluster) %>%
    summarise_all(funs(mean))

```

---

```{r}
#PCP
melted_df <- scaled_clus_avg %>% 
    gather(key=variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(Cluster),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")

```


## Check categorical vars 

Most common profiles

```{r, fig.height=8}

get_mode <- function(v) {
    uniqv <- unique(v)
    uniqv[which.max(tabulate(match(v, uniqv)))]
}

data_clus %>%
    select(-rowid) %>%
    group_by(Cluster) %>%
    summarise_all(funs(get_mode(.))) %>%
    data.frame()


```

---

```{r}
data %>%
    group_by(owns_ethereum) %>%
    summarise(n = n(), nperc = n/nrow(data)*100)

data %>%
    group_by(owns_ethereum, donated) %>%
    summarise(n = n(), nperc = n/nrow(data)*100)

```

---

```{r}
# % of Male
data %>%
 group_by(owns_ethereum, sex) %>%
 summarise(n = n(), nperc = n/nrow(.)*100) 

data %>%
  #select(-rowid) %>%
  count(sex, owns_ethereum) %>%
  group_by(owns_ethereum) %>% # to find what prop of true are female
  mutate(prop = n/sum(n))

``` 

---

```{r}
# viz
# numerical vars
library(GGally)
data_clus %>%
    select(where(is.numeric)) %>%
    ggpairs(aes(color = as_factor(Cluster), alpha = 0.5))

```

