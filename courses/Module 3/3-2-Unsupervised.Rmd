---
title: "Unsupervised Learning  <br> <br> <br>"
author: ""
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    css: [default, default-fonts, classroom.css]
    lib_dir: libs
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center")

```

## Outline

- Group the ethereum holders dataset using k means clustering
- Evaluation metrics and analysis of number of clusters
- Visualization of clusters
- Other Methods: k means, pam, hclust - dendograms?

---

## Glossary

- Observation
- Cluster

---

## Objective

We are given data from a survey of adults adapted from the census. We would like to understand characteristics of the Ethereum holders in the data. One way of understanding the kinds of Ethereum holders is to categorize them into groups based on how similar they are to each other.  

Since we do not currently have the categories, this is an unlabelled dataset. When given unlabelled data, we use can unsupervised methods to cluster or find groups within the dataset.

---

## Dataset

Let us read in the dataset, filter it to take only the Ethereum owners and prepare it by removing any NA values


```{r, message = FALSE, warning = FALSE}
library(tidyverse)

load("data_new.RDS")

data_clus <- data_new %>%
  filter(owns_ethereum == "TRUE") %>%
  mutate(donated = as.numeric(as.logical(donated))) %>% # change to 0 and 1 
  select(where(is.numeric)) %>% # --TODO save this in a csv so that user doesn't filter
  na.omit()

```

---

## K means clustering

K means is a technique that groups data points or observations into k groups in the data. The data is grouped based on feature similarity.

The algorithm allocates k centroids to the tries to minimize the distance from each data point to the centroid. In other words it tries to find the most similar data points around a representative centroid.

The formal algorithm is as follows:

- Choose the number of clusters k
- Select k random points from the data as cluster centroids
- Assign all the other surrounding data points to the closest centroid (SSE explanation)
- Recompute the centroids of newly formed clusters
- Repeat the 3rd and 4th steps till the data points remain in the same cluster/ till the centroids do not change. 
  
Alternatively we can have a stopping criteria of maximum iterations. For e.g, 100 iterations 

---

## Similarity

The concept of similarity is important in understanding how the input data is grouped. Each data point can be visualized as a vector or a collection of numeric values.

TODO 
SSE-For our clusters to be effective we need the observations in different clusters to be very different and the observations within the same cluster to be very similar. 
---

## K means

Here we carry out a k means analysis using the numeric variables

TODO 
- why scale

```{r}
set.seed(1234)

# normalize numeric values
mymat <- as.data.frame(sapply(data_clus, function(x) { (x - min(x)) / (max(x) - min(x)) }))

myclust <- kmeans(x = mymat, centers = 3)

summary(myclust)
# add the cluster assignment
data_clus$Cluster <- myclust$cluster

```

---

## Deciding the number of clusters

'k' is commonly decided by trying different numbers of groups. We decide to decrease k if there are many highly similar groups or increase the k if we think there should be more variety of groups.

Another way to visually decide on the number of clusters is to use the elbow method, where we create multiple clusters and extract the total within-cluster sum of squares value from each model. 

We want to know the value of 'k' that decreases the within-cluster sum of squares the most (has the clusters with individuals most like each other), at the same time the clusters are not redundant. This is commonly found at the elbow of the plot, which is the last value of k before the slope levels off.

```{r eval=FALSE}
# run many models with varying value of k 
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = mymat, centers = k)
  model$tot.withinss
})
# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)
# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() + geom_point()+
  scale_x_continuous(breaks = 1:10)
```

---

## Visualize the groups

Since there are multiple features in the data, we can't easily visualize how good the clusters are by using scatterplots. One way to visualize the clusters is by using tables.

```{r}
## Produce group averages
data_clus %>%
    group_by(Cluster) %>%
    mutate(Size = n()) %>%
    summarise_all(funs(mean)) %>%
    round()
```

---

### Parallel Coordinate Plot


This is another way to create side by side visualizations to compare the average feature values in each cluster
Similar to how we needed to scale the data before clustering, we need to make sure the values are of asimilar scale so that we can compare oroportions


```{r}
mymat$Cluster <- myclust$cluster 

scaled_clus_avg <- mymat %>%
    group_by(Cluster) %>%
    mutate(Size = n()) %>%
    unite(col = Cluster, Cluster, Size, sep = " size:") %>%
    group_by(Cluster) %>%
    summarise_all(funs(mean))

```

---

```{r eval=FALSE}
#PCP
melted_df <- scaled_clus_avg %>% 
    gather(key=variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(Cluster),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")

```

---
```{r echo=FALSE}
#PCP
melted_df <- scaled_clus_avg %>% 
    gather(key=variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(Cluster),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")

```

---

## 

Most common profiles

```{r, fig.height=8}

get_mode <- function(v) {
    uniqv <- unique(v)
    uniqv[which.max(tabulate(match(v, uniqv)))]
}

data_clus %>%
    group_by(Cluster) %>%
    summarise_all(funs(get_mode(.))) %>%
    data.frame()

```


---

```{r}
library(GGally)
data_clus %>%
    ggpairs(columns = 1:6, aes(color = as_factor(Cluster), alpha = 0.5))
```

---

## HCLUST

```{r}


```
---

## Your Turn

Find well separated groups from the entire dataset.

- How many different clusters have you settled at?
- Are you able to see which features distinguish the ethereum holders from the non ethereum holders?

---

## Answer

```{r}
## read in data
load("data_new.RDS")

data_clus <- data_new %>%
  mutate(donated = as.numeric(as.logical(donated))) %>% # change to 0 and 1 
  select(where(is.numeric)) %>% # --TODO save this in a csv so that user doesn't filter
  na.omit()

## Clustering
set.seed(234)

# normalize numeric values
mymat <- as.data.frame(sapply(data_clus, function(x) { (x - min(x)) / (max(x) - min(x)) }))

myclust <- kmeans(x = mymat, centers = 5)

summary(myclust)

# add the cluster assignment
data_clus$Cluster <- myclust$cluster

## Produce group averages
data_clus %>%
  group_by(Cluster) %>%
    mutate(Size = n()) %>%
    summarise_all(funs(mean)) %>%
    round()
## Viz
#PCP
mymat$Cluster <- myclust$cluster 

scaled_clus_avg <- mymat %>%
    group_by(Cluster) %>%
    mutate(Size = n()) %>%
    unite(col = Cluster, Cluster, Size, sep = " size:") %>%
    group_by(Cluster) %>%
    summarise_all(funs(mean))

melted_df <- scaled_clus_avg %>% 
    gather(key=variable, value = value, 2:(ncol(.))) %>%
    mutate(Cluster = factor(Cluster),
           variable = factor(variable, levels = names(scaled_clus_avg)[-1]))

```
---
```{r, echo=FALSE}
ggplot(melted_df, aes(variable, value, color = Cluster, group = Cluster)) +
    geom_line(size = 1.3) + theme_bw() +
    facet_wrap(~Cluster, labeller = label_both) +
    scale_color_brewer(palette = "Dark2") +
    theme(strip.text = element_text(face = "bold"),
          #strip.background = element_rect(fill = "#edc0af"), 
          axis.text.x = element_text(angle=30, hjust=1, face = "bold"), 
          axis.text.y = element_text(face="bold"), 
          legend.position = "off")
```

